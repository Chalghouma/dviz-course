{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Maps\n",
    "\n",
    "Let's draw some maps. üó∫üßê\n",
    "\n",
    "Due to some issues with Bokeh vs. Jupyter Lab, please use Jupyter notebook for the later part. Make sure you have the most recent version of everything: https://altair-viz.github.io/getting_started/installation.html#quick-start-altair-notebook and have the vega nbextension: https://github.com/vega/ipyvega#install-and-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. If Altair gives errors such as `Javascript error: unable to ...`, it is possibly due to the browser blocking certain Javascript features. Changing to another browser may solve this problem. Example: if you use Chrome, you may want to use Safari instead.\n",
    "\n",
    "2. If you use Jupyter notebook, you need to add this line:\n",
    "\n",
    "`alt.renderers.enable('notebook')`\n",
    "\n",
    "However, if you use Jupyter lab, do **not** run this line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dotmap with Altair\n",
    "\n",
    "Let's start with altair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# saving data into a file rather than embedding into the chart\n",
    "alt.data_transformers.enable('json') \n",
    "\n",
    "# jupyter notebook needs this option.\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A data cleaning lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we need a dataset with geographical coordinates. This `zipcodes` dataset contains the location and zipcode of each zip code area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "zipcodes_url = data.zipcodes.url\n",
    "zipcodes = data.zipcodes()\n",
    "zipcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a critical issue with this data. Can you spot it? ZIP codes are supposed to be 5 digits right? But why do they have only three digits? Let's sample more rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, some have five digits but some have fewer. Let's check the dtype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes.zip_code.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, they are stored as integer! I know that there are zipcodes starting with '0', so they must have been chopped off. Let's load the data again, this time by specifying the correct dtype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes = data.zipcodes(dtype={'zip_code': 'category'})\n",
    "zipcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes.zip_code.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes.zip_code.apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they all have five digits now. I think this is a useful reminder about the importance of being aware of data types, and being critical about your data. (btw, I've [reported this](https://github.com/altair-viz/vega_datasets/issues/16)). \n",
    "\n",
    "Btw, you'll have fewer issues if you pass URL instead of a dataframe to `alt.Chart`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's draw it\n",
    "\n",
    "Now we have the dataset loaded and start drawing some plots. Let's say you don't know anything about map projections. What would you try with geographical data? The simplest thing is probably considering (longitude, latitude) as a Cartesian coordinate and directly plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(zipcodes_url).mark_circle().encode(\n",
    "    x='longitude:Q',\n",
    "    y='latitude:Q',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually this itself is a map projection called [Equirectangular projection](https://en.wikipedia.org/wiki/Equirectangular_projection). This projection (or almost a *non-projection*) is super straight-forward and doesn't require any processing of the data. So, often it is used to just quickly explore geographical data. As you dig deeper, you still want to think about which map projection fits your need best. Don't just use equirectangular projection without any thoughts! \n",
    "\n",
    "Anyway, let's make it look slighly better by reducing the size of the circles and adjusting the aspect ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alt.Chart(zipcodes_url).mark_circle(size=2).encode(\n",
    "    x='longitude:Q',\n",
    "    y='latitude:Q',\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, a much better way to do this is explicitly specifying that they are lat, lng coordinates by using `longitude=` and `latitude=`, rather than `x=` and `y=`. If you do that, altair automatically adjust the aspect ratio. Try to change the width and height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(zipcodes_url).mark_circle(size=2).encode(\n",
    "    longitude='longitude:Q',\n",
    "    latitude='latitude:Q',\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the [American empire is far-reaching and complicated](https://www.youtube.com/watch?v=ASSOQDQvVLU), the information density of this map is very low (although interesting). A common projection for visualizing US data is [AlbersUSA](https://bl.ocks.org/mbostock/5545680), which uses [Albers (equal-area) projection](https://en.wikipedia.org/wiki/Albers_projection). This is a standard projection used in United States Geological Survey and the United States Census Bureau. Albers USA is simply a composition of US main land, Alaska, and Hawaii. \n",
    "\n",
    "To use it, we simply call `project` method and specify which variables are `longitude` and `latitude`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(zipcodes_url).mark_circle(size=2).encode(\n",
    "    longitude='longitude:Q',\n",
    "    latitude='latitude:Q',\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we see the large-scale division of the zipcodes? We can use the fact that the zipcodes are hierarchically organized. That is, the first digit captures the largest area divisions. \n",
    "\n",
    "Altair provides some data transformation functionalities. One of them is extracting a substring from a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from altair.expr import datum, substring\n",
    "\n",
    "alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(\n",
    "    'first_digit', substring(datum.zip_code, 0, 1)\n",
    ").encode(\n",
    "    longitude='longitude:Q',\n",
    "    latitude='latitude:Q',\n",
    "    color='first_digit:N',\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row (`datum`), you obtain the `zip_code` variable and get the substring (imagine Python list slicing), and then you call the result `first_digit`. Now, you can use this `first_digit` variable to color the circles. Also note that we specify `first_digit` as a *nominal* variable, not quantitative, to obtain a categorical colormap. But we can also play with it too. \n",
    "\n",
    "**Q: Why don't you extract the first two digits, name it as `two_digits`, and declare that as a quantitative variable? Any interesting patterns?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw, we can also put a tooltip. Also, you can always click \"view source\" or \"open in Vega Editor\" to look at the json object that **defines** this visualization. You can embed this json object on your webpage and easily put up an interactive visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(\n",
    "    'first_digit', substring(datum.zip_code, 0, 1)\n",
    ").encode(\n",
    "    longitude='longitude:Q',\n",
    "    latitude='latitude:Q',\n",
    "    color='first_digit:N',\n",
    "    tooltip='zip_code:N'\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choropleth \n",
    "\n",
    "Let's try some choropleth now. Vega datasets have US county / state boundary data (`us_10m`) and world country boundary data (`world-110m`). You can take a look at the boundaries on GitHub (they renders topoJSON files):\n",
    "\n",
    "- https://github.com/vega/vega-datasets/blob/gh-pages/data/us-10m.json\n",
    "- https://github.com/vega/vega-datasets/blob/gh-pages/data/world-110m.json\n",
    "\n",
    "If you click \"Raw\" then you can take a look at the actual file, which is hard to read. \n",
    "\n",
    "Essentially, each file is a large dictionary with the following keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap = data.us_10m()\n",
    "usmap.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap['transform']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `transformation` is used to *quantize* the data and store the coordinates in integer (easier to store than float type numbers). \n",
    "\n",
    "https://github.com/topojson/topojson-specification#212-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap['objects'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains not only county-level boundaries (objects) but also states and land boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap['objects']['land']['type'], usmap['objects']['states']['type'], usmap['objects']['counties']['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`land` is a multipolygon (one object) and `states` and `counties` contains many geometrics (multipolygons) because there are many states (counties). We can look at a state as a set of arcs that define it. It's `id` captures the identity of the state and is the key to link to other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = usmap['objects']['states']['geometries'][1]\n",
    "state1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `arcs` referred here is defined in `usmap['arcs']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap['arcs'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems pretty daunting to work with this dataset, right? But fortunately people have already built tools to handle such data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = alt.topo_feature(data.us_10m.url, 'states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(states).mark_geoshape().properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a map of US states. :) Can you use AlbersUSA projection on this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you do the same thing with counties and draw county boundaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some county-level unemployment data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_data = data.unemployment(sep='\\t')\n",
    "unemp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has unemployment rate. When? I don't know. We don't care about data provenance here because the goal is quickly trying out choropleth. But if you're working with a real dataset, you should be very sensitive about the provenance of your dataset. Make sure you understand where the data came from and how it was processed. \n",
    "\n",
    "Anyway, for each county specified with `id`. To combine two datasets, we use \"Lookup transform\" - https://vega.github.io/vega/docs/transforms/lookup/. Essentially, we use the `id` in the map data to look up (again) `id` field in the `unemp_data` and then bring in the `rate` variable. Then, we can use that `rate` variable to encode the color of the `geoshape` mark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(us_counties).mark_geoshape().project(\n",
    "    type='albersUsa'\n",
    ").transform_lookup(\n",
    "    lookup='id',\n",
    "    from_=alt.LookupData(unemp_data, 'id', ['rate'])\n",
    ").encode(\n",
    "    color='rate:Q'\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, a basic choropleth map. üòé \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster visualization with datashader\n",
    "\n",
    "Although many geovisualizations use vector graphics, raster visualization is still useful especially when you deal with images and lots of datapoints. Datashader is a package that aggregates and visualizes a large amount of data very quickly. Given a *scene* (visualization boundary, resolution, etc.), it quickly aggregate the data and produce **pixels** and send them to you. \n",
    "\n",
    "To appreciate its power, we need a fairly large dataset. Let's use NYC taxi trip dataset on Kaggle: https://www.kaggle.com/kentonnlp/2014-new-york-city-taxi-trips You can download even bigger trip data from NYC open data website: https://opendata.cityofnewyork.us/data/\n",
    "\n",
    "Ah, and you want to install the datashader, bokeh, and holoviews first if you don't have them yet.  \n",
    "\n",
    "    pip install datashader bokeh holoviews\n",
    "\n",
    "or \n",
    "\n",
    "    conda install datashader bokeh holoviews\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datashader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e899935e9f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatashader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatashader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_functions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolorcet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datashader'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datashader as ds\n",
    "from datashader import transfer_functions as tf\n",
    "from colorcet import fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is pretty big, let's use a small sample first (`nrows=10000`). For this visualization, we only keep the dropoff location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi = pd.read_csv('~/Downloads/nyc_taxi_data_2014.csv', \n",
    "                      nrows=10000, \n",
    "                      usecols=['dropoff_longitude', 'dropoff_latitude'])\n",
    "nyctaxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset is different, we can still follow the example here: http://datashader.org/getting_started/1_Introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = ds.Canvas().points(nyctaxi, 'dropoff_longitude', 'dropoff_latitude')\n",
    "tf.set_background(tf.shade(agg, cmap=fire),\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't we see anything? Wait, do you see the small dots on the left top? Can that be New York City? Maybe we don't see anything because some people travel very far? or because the dataset has some missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's no NaN in the data. let's try histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nyctaxi.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, there seems to be some outliers, which look like zero. Because lat lng should vary within a very small range, let's filter the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nyctaxi[nyctaxi['dropoff_latitude'].between(40.5, 41) & nyctaxi['dropoff_longitude'].between(-74.1, -73.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the histograms look pretty reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agg = ds.Canvas().points(df, 'dropoff_longitude', 'dropoff_latitude')\n",
    "tf.set_background(tf.shade(agg, cmap=fire), \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the black empty space at the center? That looks like the Central Park. This is cool, but it'll be awesome if we can explore the data interactively. \n",
    "\n",
    "Datashader can work with `holoviews` and `bokeh` libraries to enable some interactivity. See http://holoviews.org/gallery/apps/bokeh/nytaxi_hover.html#bokeh-gallery-nytaxi-hover But, we'll not try to reproduce this because it's a bit too much work, and because, at this moment, it's not easy to install `geoviews` and `cartopy`. Let's do a simpler version. First import the libraries and set it up to use bokeh, which is like a backend for drawing objects with javascript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%output size=200\n",
    "\n",
    "points = hv.Points(df, ['dropoff_longitude', 'dropoff_latitude'])\n",
    "taxi_trips = datashade(points, cmap=fire)\n",
    "taxi_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable dragging (four-directional arrows) and wheel-zoom (a scroll wheel and a magnifying glass) on the right side of the figure and explore the data by dragging and zooming in and out. \n",
    "\n",
    "Pretty neat, right? \n",
    "\n",
    "Ok, now let's get serious by loading the whole dataset (it may take some time btw). Drop NaN rows, and filter using the latitude and longitude range as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you feed the data directly to datashader to reproduce the static plot, this time with the full data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's fast. Also it looks cool! \n",
    "\n",
    "Now with the holoviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: how many rows (data points) are we visualizing right now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working with ?? rows (points) right now. Yet, datashader + holoviews + bokeh renders everything almost in real time! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map overlays\n",
    "\n",
    "Another useful types of visualization is overlaying data on an explorable map (Google maps, Open streetmap, ...). [Leaflet.js](https://leafletjs.com) is one of the easiest options to do that on the web, and there is a Python bridge of it: https://github.com/jupyter-widgets/ipyleaflet  Although we will not go into details, it's certainly something that's worth checking out if you're using geographical data. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
